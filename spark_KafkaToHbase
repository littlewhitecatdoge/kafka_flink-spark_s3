package com.mycompany.app;

import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory, Put, BufferedMutator}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}

import org.apache.spark.SparkContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf

import org.apache.spark.internal.Logging
import scala.collection.JavaConversions.mapAsScalaMap
import org.apache.hadoop.hbase.client.{ConnectionFactory, Put, Table}
import org.apache.spark.streaming.kafka010._
import org.apache.spark._

import org.apache.hadoop.hbase.util.Bytes

import org.apache.spark.rdd.RDD
import scala.util.parsing.json.JSON


object KafkaStreamingToHbase extends Logging{
  def main(args: Array[String]): Unit = {
    //创建streamingcontext
    val sparkConf = new SparkConf().setAppName("KafkaStreaming2Hbase")
    val ssc = new StreamingContext(sparkConf,Seconds(5))

    //准备参数
    val kafkaParams = Map[String,Object](
      "bootstrap.servers" -> "b-1.emrtest.uvqod3.c7.kafka.us-west-2.amazonaws.com:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "use_a_separate_group_id_for_each_stream",
      "auto.offset.reset" -> "latest",
      "enable.auto.commit" -> (false: java.lang.Boolean)
    )

    //准备topics
    val topics = Array("MSKTutorialTopic")
    //创建stream
    val stream = KafkaUtils.createDirectStream(
      ssc,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams)
    )

    
  
    
    stream.foreachRDD(rdd => {
      
    
      rdd.map(record1 => record1.value().toString).foreachPartition(println)
      
      rdd.foreachPartition(partitionRecords => {
        
          val conf = HBaseConfiguration.create()
          conf.set("hbase.zookeeper.quorum", "172.31.22.33")
          conf.set("hbase.zookeeper.property.clientPort", "2181")
          val conn = ConnectionFactory.createConnection(conf)

          val table = conn.getTable(TableName.valueOf("vehicle_data2"))

          partitionRecords.foreach(record => {

            val data = record.value.split(",").map(_.trim)

            val rowKey = data(0)


            val put = new Put(Bytes.toBytes(rowKey))
            put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("vehicle_id"), Bytes.toBytes(data(1)))
            put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("status"), Bytes.toBytes(data(2)))
            put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("start_time"), Bytes.toBytes(data(3)))
            put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("end_time"), Bytes.toBytes(data(4)))
            put.addColumn(Bytes.toBytes("location"), Bytes.toBytes("latitude"), Bytes.toBytes(data(5)))
            put.addColumn(Bytes.toBytes("location"), Bytes.toBytes("longitude"), Bytes.toBytes(data(6)))

            table.put(put)
            
            
      
        })
        
        table.close()
        conn.close()
   
        
    
    })
    })
    ssc.start()
    ssc.awaitTermination()

  }
    
  

}

